{
  "kind": "build_request",
  "title": "Integrate rules-based scene emotion detection into Create flow (frontend)",
  "priority": "normal",
  "requirements": [
    {
      "id": "REQ-1",
      "text": "Implement a deterministic, rules-based emotion classifier in the frontend that outputs the strict schema: emotion (fear|joy|calm|sadness|anger|surprise|wonder), intensity (0.0–1.0), energy (low|medium|high), and mood (dark|neutral|bright), derived solely from the user’s scene text (no external LLM/API calls).",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-user-last"
        ],
        "quotes": [
          "proceed with integrating the rules‑based emotion"
        ]
      },
      "acceptanceCriteria": [
        "A new frontend utility module exists (e.g., under frontend/src/utils/) that exposes a function to classify a scene text into {emotion,intensity,energy,mood}.",
        "Classification is fully deterministic and runs entirely in the browser (no network calls).",
        "Returned values are constrained to the allowed enums and intensity is always clamped to [0.0, 1.0]."
      ]
    },
    {
      "id": "REQ-2",
      "text": "Integrate the rules-based emotion classifier into the Create flow by replacing the current backend emotion-timeline generation call in the preview pipeline with a frontend-generated emotion timeline suitable for driving the virtual actor facial animation.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-user-last"
        ],
        "quotes": [
          "proceed with integrating the rules‑based emotion"
        ]
      },
      "acceptanceCriteria": [
        "frontend/src/components/ScenePreview.tsx no longer depends on the backend useGenerateEmotionTimeline() mutation for emotion generation.",
        "The preview pipeline produces an EmotionTimeline[] used by <Scene3D ... emotionTimeline={...} /> and the avatar facial animation responds to it via existing VirtualActor logic.",
        "The emotion timeline generation works for any input text (including short/edge-case strings) without throwing and always produces a valid array (may be a single cue at time 0)."
      ]
    },
    {
      "id": "REQ-3",
      "text": "Use the detected emotion outputs (emotion, intensity, energy, mood) to bias scene timing and generation parameters in the Create pipeline: apply an emotion duration multiplier to computed durations, and use energy/mood to influence music/narration and/or scene rendering parameters in a deterministic way.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-user-last"
        ],
        "quotes": [
          "proceed with integrating the rules‑based emotion"
        ]
      },
      "acceptanceCriteria": [
        "ScenePreview’s duration-related computations are adjusted by an emotion-based duration multiplier mapping: calm×1.3, wonder×1.2, joy×1.1, sadness×1.1, fear×0.8, anger×0.7, surprise×0.6 (default×1.0).",
        "At least one existing generation step is deterministically influenced by energy and/or mood (e.g., background music synthesis parameters, lighting/fog/environment in Scene3D, or narration duration), and the change is visible/testable by providing different texts (e.g., calm vs fear).",
        "No backend schema changes are required to support these adjustments."
      ]
    },
    {
      "id": "REQ-4",
      "text": "Persist the rules-based emotion analysis as part of the saved scene configuration JSON payload stored via addSceneConfig, alongside the existing gestures/emotions fields, so the backend receives the final emotion-annotated scene object data.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-user-last"
        ],
        "quotes": [
          "proceed with integrating the rules‑based emotion"
        ]
      },
      "acceptanceCriteria": [
        "When saving a scene config in ScenePreview.saveToBackend(), the sceneData JSON includes the detected emotion analysis object (emotion/intensity/energy/mood) in addition to the existing emotions timeline array.",
        "The saved JSON remains valid and does not break existing reads (backend continues treating sceneData as opaque Text).",
        "Generated videos remain savable and viewable in the library after this change."
      ]
    }
  ],
  "constraints": [
    "Do not call OpenAI or any external LLM service from the Internet Computer app (frontend or backend).",
    "Keep all backend logic in the existing single Motoko actor (backend/main.mo); do not introduce additional backend services.",
    "Do not edit files under frontend/src/components/ui or any paths listed as immutable in SYSTEM_CONTEXT."
  ],
  "nonGoals": [
    "Implementing true LLM-based emotion detection.",
    "Adding new backend endpoints or changing the Motoko EmotionTimeline type/schema.",
    "Building a multi-scene sentence splitter or full AI job orchestration/queue system beyond integrating rules-based emotion into the current Create preview pipeline."
  ],
  "imageRequirements": {
    "required": [],
    "edits": []
  },
  "userProfileUpdate": {
    "goalsToAdd": [],
    "goalsToRemove": [],
    "preferencesToAdd": [],
    "preferencesToRemove": [],
    "miscToAdd": [],
    "miscToRemove": [],
    "fieldsToSet": {},
    "fieldsToDelete": []
  }
}